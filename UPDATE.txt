V0.0.9: Add n_repeats and deploy all the codes onto HPC.

1. Add config.py, sbatch.sh and main_all.py
    Install Cython==0.29.32 and ruamel.yaml==0.17.21
2. Add more subplots (e.g., time, perplexity, and kl) into the performance metrics

3. Add n_repeats and more init_percents


V0.0.8: Update the dataset and normalize the weights for computing Y.

1. Update the dimension for '3gaussians-10dims'
    e.g., mu = r.uniform(low=0, high=5, size=n_cluster) -> mu = r.uniform(low=0, high=5, size=10)
2. Normalize the weights for computing the initial embeddings (e.g., Y)
    e.g., in inc_tsne.py
        w_sum = np.sum(w, axis=1).reshape((-1, 1))
		w = w/w_sum  # n
3. Add more evalutation metrics and plot the shepard diagram.
    e.g., shepard diagram goodness, and normalized stress



V0.0.7: Use C_P as w to initialize new Y and compute trustworthiness (trust_diff) with tsne.X_embedding and inc_tsne.X_embedding

1. Update once only
    Add main_inc_tsne2.py
2. Compute trustworthiness (trust_diff) with tsne.X_embedding and inc_tsne.X_embedding
    Also, add trust_diff into the plot
3. Use C_P (instead of P directly) as w to initialize new Y
    # w = squareform(self.P)[-n_batch:, :-n_batch]  # p_ij = (p_i|j + p_j|i)/(2N)
	w = self.C_P[-n_batch:, :-n_batch]
4. Show each iteration duration and KL error
    _show_each_iteration_duration(tsne, inc_tsne, out_dir, idx)


V0.0.6-3: Recompute P for every update from scratch without updating distance accumulatively.

1. recompute P for every update from scratch without updating distance accumulatively.


V0.0.6-2: Update the way to compute n_init_iter and n_update_iters

1. Update the way to compute n_init_iter and n_update_iters to make the total number of iterations equals to n_iters.
    E.g., changing np.ceil() to np.round() and n_update_iters
2. Increase the number of batches to use is_recompute_P during the update phase.
    is_recompute_P: if we recompute P matrix from scratch.
3. Add plt.close() to release memory and obtain fig.number
    E.g., print(f'figure number: {plt.gcf().number}, {fig.number}')


V0.0.6: Plot the result for each iteration.

1. Store each iteration's data and plot them.
2. Add get_colors to obtain colors for array-like data.
    E.g.,
        cmap = plt.get_cmap(name='rainbow')     # get cmap
        norm = matplotlib.colors.Normalize(vmin=min(y), vmax=max(y))
        colors = cmap(norm(y), bytes=True)   # RGBA. bytes=True to get integer for each color value
3. Change "gif" to "mp4"
    e.g., imageio.v2.mimsave(out_file, images, format='mp4', **kwargs)  # each image 0.5s = duration/n_imgs


V0.0.5-4: Update init_iters and update_iters

1. Update init_iters = (75, 225), where 1000 * 0.3 = 300 = 75+225 (250:750=75:225)
   Update update_iters = (35, 105), where 1000 * 0.7 = 700 = 5 batches * (35+105) (here, we have 5 batches)
2. Set is_optimized = True
3. Install perplexity.pyx (pip install .)


V0.0.4-1: Add single neural network

1. Add single neural network with different attempts.
2. Add anim.py


1. Implement perplexity to compute sigma for KL
2. Add 5 and 10 clusters with random centroids in 5 dimensions


V0.0.3-4: Add perplexity for KL

1. Implement perplexity to compute sigma for KL
2. Add 5 and 10 clusters with random centroids in 5 dimensions


V0.0.3-3: Add fr.sh

1. Add fr.sh for hpc
2. Add plt.savefig() for single_nn.py and single_nn2.py


V0.0.3-2: Add 5 Gaussians with 5 dimensions

1. Update loss function for overflowing
    torch.clamp((p_ij * torch.log(p_ij / q_ij + 1e-10).abs()), min=-1e-20, max=1e+20)
2. Add 5 Gaussians with 5 dimensions


V0.0.3: Using a single point as the input for Neural network

1. Add tsne_noise.py
2. Try different pair_nn.
3. Add single_nn.py with KL divergence + Euclidean distance + Cosine similarity.
